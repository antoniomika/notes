{"componentChunkName":"component---src-gatsby-theme-code-notes-templates-note-js","path":"/zfs-nas/","result":{"data":{"mdx":{"body":"var _excluded = [\"components\"];\nfunction _extends() { _extends = Object.assign ? Object.assign.bind() : function (target) { for (var i = 1; i < arguments.length; i++) { var source = arguments[i]; for (var key in source) { if (Object.prototype.hasOwnProperty.call(source, key)) { target[key] = source[key]; } } } return target; }; return _extends.apply(this, arguments); }\nfunction _objectWithoutProperties(source, excluded) { if (source == null) return {}; var target = _objectWithoutPropertiesLoose(source, excluded); var key, i; if (Object.getOwnPropertySymbols) { var sourceSymbolKeys = Object.getOwnPropertySymbols(source); for (i = 0; i < sourceSymbolKeys.length; i++) { key = sourceSymbolKeys[i]; if (excluded.indexOf(key) >= 0) continue; if (!Object.prototype.propertyIsEnumerable.call(source, key)) continue; target[key] = source[key]; } } return target; }\nfunction _objectWithoutPropertiesLoose(source, excluded) { if (source == null) return {}; var target = {}; var sourceKeys = Object.keys(source); var key, i; for (i = 0; i < sourceKeys.length; i++) { key = sourceKeys[i]; if (excluded.indexOf(key) >= 0) continue; target[key] = source[key]; } return target; }\n/* @jsxRuntime classic */\n/* @jsx mdx */\n\nvar _frontmatter = {\n  \"title\": \"Roll you own ZFS NAS\",\n  \"tags\": [\"blog\"],\n  \"emoji\": \"\",\n  \"link\": \"\",\n  \"date\": \"2024-03-13\"\n};\nvar layoutProps = {\n  _frontmatter: _frontmatter\n};\nvar MDXLayout = \"wrapper\";\nreturn function MDXContent(_ref) {\n  var components = _ref.components,\n    props = _objectWithoutProperties(_ref, _excluded);\n  return mdx(MDXLayout, _extends({}, layoutProps, props, {\n    components: components,\n    mdxType: \"MDXLayout\"\n  }), mdx(\"h2\", {\n    \"id\": \"background\"\n  }, \"Background\"), mdx(\"p\", null, \"Recently I've noticed an uptick in the engagement found within the self-hosting community,\\nso I've decided to start a series of posts that look into why and how I handle self-hosting.\\nToday's post will focus on data storage and will look into some of the options available when\\nit comes to data backups.\"), mdx(\"p\", null, \"Data storage is hard. Data backup is even harder. Therefore, I spent some time recently\\nto re-evaluate my backup strategy. Prior to deciding to roll my own backup solution, I would generally\\nbackup files to Google Drive as my main \\\"backup\\\" mechanism. This was quite a shameful setup but gave\\nme a good amount of storage with easy access to all of my data. I used the Enterprise Workspace plan\\nwhich gave me access to as much storage as I needed, but Google soon changed their offering. I was using\\n~9TB of storage at that time, so once they removed the \\\"as much as you need\\\" provision, I had to use 2 users\\nworth of pooled storage. This amounts to ~$40/mo, which is still not terrible for data storage that is\\nfairly reliable.\"), mdx(\"h2\", {\n    \"id\": \"its-as-easy-as-3-2-1\"\n  }, \"It's as easy as 3-2-1\"), mdx(\"p\", null, \"When architecting my new backup strategy, I decided it was time for an upgrade. Generally, the 3-2-1\\ndata backup method is recommended. The idea with this strategy is you maintain \", mdx(\"em\", {\n    parentName: \"p\"\n  }, \"3\"), \" different copies of\\nyour data, with \", mdx(\"em\", {\n    parentName: \"p\"\n  }, \"2\"), \" copies stored in two different locations/media, and \", mdx(\"em\", {\n    parentName: \"p\"\n  }, \"1\"), \" copy stored at an offsite location.\\nThis setup is pretty easy to achieve and provides pretty good fault-tolerance and disaster recovery.\\nIt also ensures that your data is protected when the unthinkable happens.\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-text\"\n  }, \"|--------|    |---------|    |---------|\\n|   3    | -> | 2 Media | -> |    1    |\\n| Copies |    |  Types  |    | Offsite |\\n|--------|    |---------|    |---------|\\n\")), mdx(\"p\", null, \"Achieving this backup strategy isn't particularly difficult to do. A simple setup with this scheme could\\nbe done with the following:\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Primary data source (a laptop)\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"A backup of the primary data source (a usb or external hard drive)\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"A backup of the primary data source (a cloud backup)\")), mdx(\"p\", null, \"With a setup like this, we end up with 3 copies of our data. We have at least 2 different types of media\\n(external hard drive and cloud storage), and one copy offsite (in the cloud). Therefore, we should have\\nfairly decent data redundancy.\"), mdx(\"h2\", {\n    \"id\": \"my-strategy\"\n  }, \"My strategy\"), mdx(\"p\", null, \"Based on the above and for my own purposes, I decided a viable backup process would involve the following:\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Primary data source (laptops, desktops, phones, etc)\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"A backup of the primary data to the NAS at home\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"A backup of the NAS at home to a similar NAS offsite\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"A backup of the NAS to the cloud\")), mdx(\"p\", null, \"This scheme gives me a decent amount of flexibility and options for backing up my data, as well as generally\\nfollows the 3-2-1 rule I described above. The main benefit of using this method is that each device I backup only\\nneeds to keep track of a single backup target. That backup target then can be easily backed up to a secondary target\\nwithout the primary device needing to have any intervention. In the event the backup target is destroyed, it can be\\nreplaced by the secondary target, and the secondary target replaced by a new device with all of the data replicated\\nto it. This ensures that in the event a device is lost, data is still well protected and devices can be replaced easily\\nwith minimal downtime since we can promote devices to take each other's place as needed.\"), mdx(\"h2\", {\n    \"id\": \"technologies\"\n  }, \"Technologies\"), mdx(\"p\", null, \"Primary data sources would be backed up using the following:\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://www.borgbackup.org/\"\n  }, \"Borg\"), \" (via \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://torsion.org/borgmatic/\"\n  }, \"Borgmatic\"), \" or \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://vorta.borgbase.com/\"\n  }, \"Vorta\"), \") for linux/macos hosts using\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://rsync.samba.org/\"\n  }, \"rsync\"), \" for random hosts/data that don't need dedupe and other Borg niceties\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://www.samba.org/\"\n  }, \"samba\"), \" for macOS/Windows\")), mdx(\"p\", null, \"The backup targets would be machines running \", mdx(\"em\", {\n    parentName: \"p\"\n  }, \"Ubuntu Server 22.04 LTS\"), \". All backup data would be stored in \", mdx(\"em\", {\n    parentName: \"p\"\n  }, \"ZFS\"), \",\\nwhich would ultimately make our desired scheme trivial to implement. They would have the following configuration:\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"32gb of ram\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"8 core cpu, 3.5ghz base clock\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"4 18tb HDDs using ZFS and in a single zpool\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"A small HDD for OS install\")), mdx(\"p\", null, \"For the base OS, the default installation parameters were chosen. Regarding the actual backup storage devices (the 18tb HDDs),\\na zpool was created that consisted of two mirrored vdevs, with each mirror containing 2 disks. This strategy provides decent\\nredundancy in the case that a disk fails (we can lose up to one disk in each vdev), while also allowing us to grow the pool in\\nthe future. If the pool is ever running low on data, we can easily add another vdev of 2 disks to increase the capacity. This\\nmethod does result in our storage pool having capacity of half the total disk space we have available (18tb * 2 vdevs = 36tb).\"), mdx(\"p\", null, \"Over using zraid, this option gives us fantastic performance, good scalability, and ease of management.\"), mdx(\"p\", null, \"The choice of ZFS simplifies our NAS backups, as we can utilize the ability of ZFS to send and receive snapshots to send backups\\nof our data. This is a huge benefit as it simplifies the backup process tremendously. Our systems are large enough that the overhead\\nof running ZFS itself should be neglible, and we can reap huge benefits in our ability to easily replicate our data. Snapshots don't\\ncost us anything to use (a huge benefit due to the fact that ZFS is CoW, copy-on-write), so we can feel safe knowing that we can use them.\"), mdx(\"h2\", {\n    \"id\": \"backup-setup\"\n  }, \"Backup Setup\"), mdx(\"h3\", {\n    \"id\": \"zfs-setup\"\n  }, \"ZFS Setup\"), mdx(\"p\", null, \"The setup process for each NAS was pretty much the same and can be summarized by the following:\"), mdx(\"ol\", null, mdx(\"li\", {\n    parentName: \"ol\"\n  }, mdx(\"p\", {\n    parentName: \"li\"\n  }, \"Install ZFS on Linux and setup the zpool named \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"backup\"), \":\"), mdx(\"pre\", {\n    parentName: \"li\"\n  }, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-bash\"\n  }, \"# Install ZoL\\napt-get update && apt-get install zfsutils-linux -y\\n\\n# Get the list of devices by their ids to ensure\\n# they are found correctly when the pool is imported:\\nls /dev/disk/by-id/*\\n\\n# Create the mirrored pool with the first vdev\\nzpool create -o ashift={ashift} backup mirror \\\\\\n  /dev/disk/by-id/{device_id_here} \\\\\\n  /dev/disk/by-id/{device_id_here}\\n\\n# Add another vdev to the pool (can be done as many times as we want, expanding the pool)\\nzpool add -o ashift={ashift} backup mirror \\\\\\n  /dev/disk/by-id/{device_id_here} \\\\\\n  /dev/disk/by-id/{device_id_here}\\n\\n# Enable compression for the pool (if desired)\\nzfs set compression=on backup\\n\\n# Disable mounting for the pool (if desired)\\nzfs set canmount=off backup\\n\")), mdx(\"blockquote\", {\n    parentName: \"li\"\n  }, mdx(\"p\", {\n    parentName: \"blockquote\"\n  }, mdx(\"strong\", {\n    parentName: \"p\"\n  }, mdx(\"em\", {\n    parentName: \"strong\"\n  }, \"NOTE:\")), \" I decided to use \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"compression=on\"), \", but you can tune this to your own preferences.\\nI also decided not to encrypt the entire zpool, so I could control this per-dataset (and therefore),\\nhave different encryption keys per dataset. You should modify these snippets how you want (including)\\nchanging variables to what you want them to be.\"))), mdx(\"li\", {\n    parentName: \"ol\"\n  }, mdx(\"p\", {\n    parentName: \"li\"\n  }, \"Setup the different datasets you want:\"), mdx(\"pre\", {\n    parentName: \"li\"\n  }, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-bash\"\n  }, \"# Create an encrypted dataset for borg\\nzfs create -o encryption=aes-256-gcm \\\\\\n  -o keylocation=prompt \\\\\\n  -o keyformat=passphrase \\\\\\n  backup/borg\\n\\n# Create an encrypted dataset for misc\\nzfs create -o encryption=aes-256-gcm \\\\\\n  -o keylocation=prompt \\\\\\n  -o keyformat=passphrase \\\\\\n  backup/misc\\n\\n# Setup a dataset for samba with some settings we need\\n# We disable access times, inherit acls, disable unneeded\\n# permissions, and set extended attributes to be stored more\\n# optimally for performance. I also set a quota for samba\\n# and the descendant data sets to 5T.\\n# The quota can also be changed later or switched to `refquota`\\n# which does not include snapshot sizes.\\nzfs create -o encryption=aes-256-gcm\\n  -o keylocation=prompt\\n  -o keyformat=passphrase\\n  -o atime=off \\\\\\n  -o dnodesize=auto \\\\\\n  -o aclinherit=passthrough \\\\\\n  -o acltype=posixacl \\\\\\n  -o xattr=sa \\\\\\n  -o exec=off \\\\\\n  -o devices=off \\\\\\n  -o setuid=off \\\\\\n  -o canmount=on \\\\\\n  -o quota=5T \\\\\\n  backup/samba\\n\\n# Setup a dataset for windows and inherit the samba configs\\n# (but set a different encryption key)\\nzfs create -o encryption=aes-256-gcm \\\\\\n  -o keylocation=prompt \\\\\\n  -o keyformat=passphrase backup/samba/windows\\n\\n# Setup a dataset for macos and inherit the samba configs\\n# (but set a different encryption key)\\nzfs create -o encryption=aes-256-gcm \\\\\\n  -o keylocation=prompt \\\\\\n  -o keyformat=passphrase backup/samba/macos\\n\\n# Setup a dataset for public use and inherit the samba configs\\n# (but set a different encryption key)\\nzfs create -o encryption=aes-256-gcm \\\\\\n  -o keylocation=prompt \\\\\\n  -o keyformat=passphrase backup/samba/public\\n\")))), mdx(\"p\", null, \"After running the above, we can see the status of our pool:\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-bash\"\n  }, \"# Get the zpool status\\nzpool status\\n\")), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-bash\"\n  }, \"  pool: backup\\n state: ONLINE\\n  scan: scrub repaired 0B in 08:40:45 with 0 errors on Sun Mar 10 09:04:46 2024\\nconfig:\\n\\n        NAME                                  STATE     READ WRITE CKSUM\\n        backup                                ONLINE       0     0     0\\n          mirror-0                            ONLINE       0     0     0\\n            {device_id_here}                  ONLINE       0     0     0\\n            {device_id_here}                  ONLINE       0     0     0\\n          mirror-1                            ONLINE       0     0     0\\n            {device_id_here}                  ONLINE       0     0     0\\n            {device_id_here}                  ONLINE       0     0     0\\n\\nerrors: No known data errors\\n\")), mdx(\"p\", null, \"And get our datasets:\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-bash\"\n  }, \"# List our datasets\\nzfs list -t filesystem\\n\")), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-bash\"\n  }, \"NAME                   USED  AVAIL     REFER  MOUNTPOINT\\nbackup                   0K  34.0T        0K  /backup\\nbackup/borg              0K  34.0T        0K  /backup/borg\\nbackup/misc              0K  34.0T        0K  /backup/misc\\nbackup/samba             0K  5.00T        0K  /backup/samba\\nbackup/samba/macos       0K  5.00T        0K  /backup/samba/macos\\nbackup/samba/public      0K  5.00T        0K  /backup/samba/public\\nbackup/samba/windows     0K  5.00T        0K  /backup/samba/windows\\n\")), mdx(\"h3\", {\n    \"id\": \"software-setup\"\n  }, \"Software Setup\"), mdx(\"p\", null, \"For software that truly isn't necessary to run on the host, I'll be utilizing\\n\", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://docs.docker.com/engine/\"\n  }, \"Docker\"), \" and \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://docs.docker.com/compose/\"\n  }, \"Docker Compose\"), \"\\nfor deployment and software management. I've decided to do this is it makes it easy for me to\\nmanage configuration state and track changes to the deployment strategy as code. Also, this ensures\\nthat any software I run on this host will continue to function even if I move to a different host OS\\n(for example, if I decide to swith to Debian or Fedora). You could also use \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://podman.io/\"\n  }, \"Podman\"), \"\\nif you'd like for this step.\"), mdx(\"blockquote\", null, mdx(\"p\", {\n    parentName: \"blockquote\"\n  }, mdx(\"strong\", {\n    parentName: \"p\"\n  }, mdx(\"em\", {\n    parentName: \"strong\"\n  }, \"NOTE:\")), \" The below settings have a user and password set with \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"${USER}\"), \" and \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"${PASSWORD}\"), \" respectively.\\nThis is not an environment variable. You need to modify these snippets yourself in order to set it up how you want it.\")), mdx(\"h4\", {\n    \"id\": \"ssh-setup-borg-and-rsync\"\n  }, \"SSH Setup (borg and rsync)\"), mdx(\"h5\", {\n    \"id\": \"borg\"\n  }, \"Borg\"), mdx(\"p\", null, \"I utilize the \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://hub.docker.com/r/nold360/borgserver\"\n  }, mdx(\"inlineCode\", {\n    parentName: \"a\"\n  }, \"nold360/borgserver\")), \" image. The image is easy to\\nconfigure, and assumes I have the local directories \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"./sshkeys\"), \" and \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"./data\"), \" to store each piece of data\\naccordingly. User ssh keys are placed in \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"./sshkeys/clients/\"), \", each being the name of the borg repository that key\\nwill have access to. It's important to note that this file can only contain a single key. Setting \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"BORG_APPEND_ONLY\"), \"\\ndisables data deletion until the \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"BORG_ADMIN\"), \" runs a prune operation. Here's the compose file:\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-yml\"\n  }, \"version: '3.8'\\nservices:\\n  server:\\n    image: nold360/borgserver:bookworm\\n    volumes:\\n      - ./sshkeys:/sshkeys\\n      - ./data:/backup\\n    ports:\\n      - \\\"22222:22\\\"\\n    environment:\\n      BORG_SERVE_ARGS: \\\"--progress --debug\\\"\\n      BORG_APPEND_ONLY: \\\"yes\\\"\\n      # BORG_ADMIN: \\\"${USER}\\\"\\n    restart: always\\n\")), mdx(\"p\", null, \"I keep the compose file at the root of the \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"/backup/borg\"), \" dataset. This allows my compose setup to also be included\\nas part of snapshots.\"), mdx(\"h5\", {\n    \"id\": \"rsync\"\n  }, \"rsync\"), mdx(\"p\", null, \"rsync access is done directly using the host in this situation. I previously used a docker image for this, but decided\\nit was unnecessary.\"), mdx(\"h4\", {\n    \"id\": \"samba-setup\"\n  }, \"Samba Setup\"), mdx(\"p\", null, \"I utilize the \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://github.com/vremenar/samba/pkgs/container/samba\"\n  }, mdx(\"inlineCode\", {\n    parentName: \"a\"\n  }, \"ghcr.io/vremenar/samba\")), \" image,\\nwhich is based on \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://hub.docker.com/r/dperson/samba\"\n  }, mdx(\"inlineCode\", {\n    parentName: \"a\"\n  }, \"dperson/samba\")), \" but updates the samba and\\nalpine versions. I then utilize a custom samba config for setting Time Machine shares, and the default\\nconfiguration provided by the image. Here's the compose file:\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-yml\"\n  }, \"version: '3.8'\\nservices:\\n  server:\\n    image: ghcr.io/vremenar/samba:latest\\n    volumes:\\n      - ./samba.conf:/samba.conf:ro\\n      - ./macos:/macos\\n      - ./public:/public\\n      - ./windows:/windows\\n    ports:\\n      - \\\"139:139\\\"\\n      - \\\"445:445\\\"\\n    command: |\\n      -p -n\\n      -g \\\"log level = 2\\\"\\n      -I /samba.conf\\n      -u \\\"${USER};${PASSWORD}\\\"\\n      -s \\\"public;/public;yes;yes;yes;all;none;${USER}\\\"\\n      -s \\\"windows-shared;/windows/shared;yes;no;no;all\\\"\\n      -s \\\"macos-shared;/macos/shared;yes;no;no;all\\\"\\n    restart: always\\n\")), mdx(\"p\", null, \"This configuration broadcasts 3 shares by default:\"), mdx(\"ol\", null, mdx(\"li\", {\n    parentName: \"ol\"\n  }, \"public, mapped to the \", mdx(\"inlineCode\", {\n    parentName: \"li\"\n  }, \"/public\"), \" volume. It is browseable (discoverable),\\nis read only, and has guest access enabled. All users have access to view\\nthe share, and there are no admins on the share. The only user that can\\nwrite files to the share is ${USER}. I'll utilize this share for storing\\npublic assets that I might need on my network (installation scripts, shared\\napps, etc).\"), mdx(\"li\", {\n    parentName: \"ol\"\n  }, \"windows-shared, mapped to \", mdx(\"inlineCode\", {\n    parentName: \"li\"\n  }, \"/windows/shared\"), \". This is a shared mount for\\nwindows machines on the network. All users have access to it and it is\\nbrowseable.\"), mdx(\"li\", {\n    parentName: \"ol\"\n  }, \"macos-shared, mapped to \", mdx(\"inlineCode\", {\n    parentName: \"li\"\n  }, \"/macos/shared\"), \". This is a shared mount for\\nmacOS machines on the network. All users have access to it and it is\\nbrowseable.\")), mdx(\"p\", null, \"There is nothing preventing mac or windows machines from accessing the shared mounts,\\nbut this allows me to set attributes per-share if needed in the future\\n(such as shadow files and versions in windows). Also, more separation is not a\\nbad thing in this situation.\"), mdx(\"p\", null, \"For Time Machine, a custom samba.conf is utilized. the contents are as follows:\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-text\"\n  }, \"[${USER}-timemachine]\\n    comment = ${USER}'s Time Machine\\n    path = /macos/timemachine/${USER}\\n    browseable = no\\n    writeable = yes\\n    create mask = 0600\\n    directory mask = 0700\\n    spotlight = yes\\n    vfs objects = catia fruit streams_xattr\\n    fruit:aapl = yes\\n    fruit:time machine = yes\\n    valid users = ${USER}\\n\")), mdx(\"p\", null, \"Here we create a non-browseable share that has a single valid user. We also set the proper \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"vfs objects\"), \"\\nsettings and mark the share as Time Machine specific.\"), mdx(\"p\", null, \"I keep the compose file and config at the root of the \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"/backup/samba\"), \" dataset. This allows my compose setup to also be included\\nas part of snapshots like the above.\"), mdx(\"p\", null, \"It's important that you set the right permissions on the path you use within the extra \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"samba.conf\"), \" (like in my situation).\\nYou need to make sure the directory exists in your zfs dataset and has the right permissions so the samba container can\\naccess it. For me, I ran the following:\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-bash\"\n  }, \"# Create the path on the ZFS dataset\\nmkdir -p macos/timemachine/${USER}\\n\\n# Set permissions on the path to the smbuser and smb group from within the container\\n# If you deploy it with a different method than me, you can use `id smbuser` to get\\n# the correct uid/gid to use.\\nchown -R 100:101 macos/timemachine/${USER}\\n\")), mdx(\"h2\", {\n    \"id\": \"replication-setup\"\n  }, \"Replication Setup\"), mdx(\"p\", null, \"Now that our NAS and different methods of getting data onto our NAS are setup, it's time to setup replication to ensure\\nour NAS is backed up to a secondary location (the last part of our 3-2-1 solution). To do this, we'll make use of ZFS\\nSnapshots, which is an easy way to take a snapshot of the current state of a dataset.\"), mdx(\"h3\", {\n    \"id\": \"zfs-snapshots\"\n  }, \"ZFS Snapshots\"), mdx(\"p\", null, \"Because ZFS is a CoW (copy-on-write) filesystem, snapshots don't utilize any extra data and are immutable.\\nSnapshots can also be sent over a pipe (like with SSH) so they are portable. If desired, snapshots could\\neven be written to file. The other powerful utility of snapshots is that we can utilize them incrementally,\\nmeaning we only send the changes to a dataset each backup cycle instead of the entire dataset.\"), mdx(\"p\", null, \"In order to take a snapshot, we utilize the \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"zfs snapshot\"), \" command like so:\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-bash\"\n  }, \"# Take a snapshot of the main backup/samba dataset. The snapshot name is `initial`.\\n# Because we use `-r`, this will also take a snapshot of all child datasets\\nzfs snapshot -r backup/samba@initial\\n\")), mdx(\"p\", null, \"After running this command, we can show our snapshots with the following command:\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-bash\"\n  }, \"# List all items from our zpool that come from backup/samba\\nzfs list -t all -r backup/samba\\n\")), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-bash\"\n  }, \"NAME                           USED  AVAIL     REFER  MOUNTPOINT\\nbackup/samba                     0K  5.00T        0K  /backup/samba\\nbackup/samba@initial             0K      -        0K  -\\nbackup/samba/macos               0K  5.00T        0K  /backup/samba/macos\\nbackup/samba/macos@initial       0K      -        0K  -\\nbackup/samba/public              0K  5.00T        0K  /backup/samba/public\\nbackup/samba/public@initial      0K      -        0K  -\\nbackup/samba/windows             0K  5.00T        0K  /backup/samba/windows\\nbackup/samba/windows@initial     0K      -        0K  -\\n\")), mdx(\"p\", null, \"If we have another zfs machine we can send our snapshots to, we can run something like so:\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-bash\"\n  }, \"# Send the snapshots verbosely (-v) under `backup/samba` with the name `initial` recursively (-R)\\n# raw (-w), meaning as the encrypted data on disk. Pipe it through `pv` (pipeviewer to see transfer stats)\\n# and receive it on the server named `backup1`, allowing for interruption (-s), also with verbose info (-v)\\n# into the dataset named backup/samba.\\nzfs send -vRw backup/samba@initial | pv | ssh backup1 zfs recv -s -v backup/samba\\n\")), mdx(\"p\", null, \"On \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"backup1\"), \", we can see the snapshots and datasets like above:\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-bash\"\n  }, \"NAME                           USED  AVAIL     REFER  MOUNTPOINT\\nbackup/samba                     0K  5.00T        0K  /backup/samba\\nbackup/samba@initial             0K      -        0K  -\\nbackup/samba/macos               0K  5.00T        0K  /backup/samba/macos\\nbackup/samba/macos@initial       0K      -        0K  -\\nbackup/samba/public              0K  5.00T        0K  /backup/samba/public\\nbackup/samba/public@initial      0K      -        0K  -\\nbackup/samba/windows             0K  5.00T        0K  /backup/samba/windows\\nbackup/samba/windows@initial     0K      -        0K  -\\n\")), mdx(\"p\", null, \"The difference is, our datasets are not mounted in each mountpoint. You can see this by running:\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-bash\"\n  }, \"# Use df to see mounted filesystems\\ndf -h\\n\")), mdx(\"p\", null, \"In order to mount them, we do the following:\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-bash\"\n  }, \"# Mount all zfs mounts, and load the encryption keys\\nzfs mount -al\\n\")), mdx(\"p\", null, \"For each filesystem, you will be asked for the passphrase that was used when the dataset was created.\\nThis means we can send our encrypted filesystems anywhere (even to a file) and not be worried that our data\\ncan be accessed. This is great for all kinds of reasons and opens up many possibilities. For example, a friend\\nand I can be each other's offsite backup without being worried of them accessing my data. You can also save snapshots\\nto a file and store them on a blob storage backend or some storage box in the cloud.\"), mdx(\"p\", null, \"Filesystems can be unmounted using the following:\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-bash\"\n  }, \"# Unmount all zfs mounts, and unload the encryption keys\\nzfs unmount -au\\n\")), mdx(\"p\", null, \"If we take another snapshot on \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"backup0\"), \" and want to send it to \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"backup1\"), \" incrementally, we can do it like so:\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-bash\"\n  }, \"# Take the snapshot\\nzfs snapshot -r backup/samba@next\\n\\n# Send snapshots between `initial` and `next` to `backup1`\\nzfs send -vRwI backup/samba@initial backup/samba@next | pv | ssh backup1 zfs recv -s -v backup/samba\\n\")), mdx(\"h3\", {\n    \"id\": \"reconciliation\"\n  }, \"Reconciliation\"), mdx(\"p\", null, \"If you see an error like this:\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-bash\"\n  }, \"send from @initial to backup/samba@next estimated size is 77.1K\\nsend from @initial to backup/samba/public@next estimated size is 43.1K\\nsend from @initial to backup/samba/macos@next estimated size is 112K\\nsend from @initial to backup/samba/windows@next estimated size is 40.6K\\ntotal estimated size is 273K\\nreceiving incremental stream of backup/samba@next into backup/samba@next\\ncannot receive incremental stream: destination backup/samba has been modified\\nsince most recent snapshot\\n86.8KiB 0:00:00 [ 164KiB/s] [<=>                                             ]                                          \\n\")), mdx(\"p\", null, \"You need to reset the state of the receiving pool to the snapshot that was previously sent.\\nYou can do that like so:\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-bash\"\n  }, \"# List snapshots (-t) of the dataset (and children, -r), without headers (-H), and roll it back\\n# This assumes there is only one snapshot per dataset on the machine, your mileage may vary.\\nzfs list -Hrt snapshot backup/samba | awk '{print $1}' | xargs -I{} zfs rollback {}\\n\")), mdx(\"p\", null, \"You can avoid this by setting each dataset on the remote backup side to readonly like so:\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-bash\"\n  }, \"zfs set readonly=on backup/samba\\n\")), mdx(\"h3\", {\n    \"id\": \"automated-backups-with-send-and-receive\"\n  }, \"Automated backups with send and receive\"), mdx(\"p\", null, \"Now that we have all of the tools we need to make backups a reality, let's go ahead and set up an automated way to handle backups.\\nWe will be using \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"backup0\"), \" as our main NAS and \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"backup1\"), \" as our secondary NAS\"), mdx(\"p\", null, \"First, let's create a user on our \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"backup1\"), \" for us to receive ssh connections to:\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-bash\"\n  }, \"# Use adduser to create a user, we can use the defaults for everything.\\nadduser zfsbackup\\n\")), mdx(\"p\", null, \"Next, let's create a SSH key on \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"backup0\"), \" which will be used to access the user:\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-bash\"\n  }, \"# Generate an ed25519 key. Save it to a file like `~/.ssh/id_ed25519_zfsbackup`\\n# You may choose to have a key per dataset as we can limit which dataset the ssh\\n# process has access to with `authorized_keys`.\\nssh-keygen -t ed25519\\n\")), mdx(\"p\", null, \"On \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"backup1\"), \", allow \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"zfsbackup\"), \" user access to the pool (or specific datasets) we want with only create, mount, and receive permissions:\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-bash\"\n  }, \"# Limit our blast radius by only allowing zfsbackup to create, mount and receive files. Don't allow it to destroy or delete data.\\nzfs allow -u zfsbackup create,mount,receive backup\\n\")), mdx(\"p\", null, \"Also on \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"backup1\"), \", let's setup the \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"authorized_keys\"), \" file in \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"/home/zfsbackup/.ssh/authorized_keys\"), \" with the following:\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-text\"\n  }, \"command=\\\"/opt/backup/ssh.sh backup/samba\\\",no-port-forwarding,no-X11-forwarding,no-agent-forwarding,no-pty ssh-ed25519 .... root@backup0\\n\")), mdx(\"p\", null, \"This file only allows the zfsbackup user to run a single command (\", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"/opt/backup/ssh.sh backup/samba ...\"), \"), which is a wrapper around \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"zfs recv\"), \" and\\nalso allows us to get the latest snapshot on the host, which is how we will only incrementally send the snapshots that \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"backup1\"), \" doesn't know\\nabout. This allows us to limit the types of commands the ssh user can run.\"), mdx(\"p\", null, \"Next, the contents of the shell script at \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"/opt/backup/ssh.sh\"), \" is as follows:\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-bash\"\n  }, \"#!/bin/bash\\n\\n# Set the dataset name the user can access\\nDATASET_NAME=\\\"${1:?Dataset not provided}\\\"\\n\\n# Go through the exec command that was sent via ssh\\ncase \\\"$SSH_ORIGINAL_COMMAND\\\" in\\n  recv)\\n    # Receive the snapshots into the dataset\\n    zfs recv -v \\\"${DATASET_NAME}\\\"\\n    ;;\\n  latest)\\n    # List the most recent snapshot in the dataset\\n    zfs list -t snapshot -o name -s creation -H \\\"${DATASET_NAME}\\\" | tail -n 1\\n    ;;\\n  *)\\n    echo \\\"unknown command $DATASET_NAME\\\"\\n    exit 1\\n    ;;\\nesac\\n\")), mdx(\"p\", null, \"This prevents the user from making queries about any datasets other than the one pinned in the \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"authorized_keys\"), \" file. This can be easily changed\\nto allow the user access to any of the datasets in a pool like so:\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-bash\"\n  }, \"#!/bin/bash\\n\\n# Set the dataset name/parent the user has access to\\nDATASET_NAME=\\\"${1:?Dataset or pool not provided}\\\"\\n\\n# Set the original command as args\\nset -- $SSH_ORIGINAL_COMMAND\\n\\n# Pull the dataset the user wanted to manage\\nREAL_DATASET=\\\"${2:-$DATASET_NAME}\\\"\\n\\n# Check if the dataset is a child of the allowed parent\\nif [[ $REAL_DATASET != $DATASET_NAME* ]]; then\\n  echo \\\"no permissions for dataset $REAL_DATASET\\\"\\n  exit 1\\nfi\\n\\n# Check the command the user wants to run\\ncase \\\"$1\\\" in\\n  recv)\\n    # Receive the snapshots\\n    zfs recv -v \\\"${REAL_DATASET}\\\"\\n    ;;\\n  latest)\\n    # List the latest snapshot for the dataset\\n    zfs list -t snapshot -o name -s creation -H \\\"${REAL_DATASET}\\\" | tail -n 1\\n    ;;\\n  *)\\n    echo \\\"unknown command $1\\\"\\n    exit 1\\n    ;;\\nesac\\n\")), mdx(\"p\", null, \"Put this script somewhere owned by \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"root\"), \" but accessible to other users (and executable). I chose \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"/opt/backup/ssh.sh\"), \". The directory\\nand file have permissions \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"0755\"), \" set on it.\"), mdx(\"blockquote\", null, mdx(\"p\", {\n    parentName: \"blockquote\"\n  }, mdx(\"strong\", {\n    parentName: \"p\"\n  }, mdx(\"em\", {\n    parentName: \"strong\"\n  }, \"NOTE:\")), \" Use these scripts at your own risk. I have not configured them to handle every possible corner case.\")), mdx(\"p\", null, \"We can test that our script is working properly by querying for the latest snapshot:\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-bash\"\n  }, \"ssh -i .ssh/id_ed25519_zfsbackup zfsbackup@backup1 latest\\n\")), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-bash\"\n  }, \"backup/samba@next\\n\")), mdx(\"p\", null, \"Now let's setup our cronjob to actually send our updates. On \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"backup0\"), \", we create a file at \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"/etc/cron.daily/backup\"), \" with the same\\npermissions \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"0755\"), \" set. I chose to use the run-parts cron setup, but you can choose to do this however you'd like.\"), mdx(\"p\", null, \"The content of the file looks like this:\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-bash\"\n  }, \"#!/bin/bash\\n\\n# Set our path\\nPATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin\\n\\n# Safe bash defaults\\nset -euo pipefail\\n\\n# Create our snapshot name based on todays iso date\\nSNAPSHOT_NAME=\\\"$(date +\\\"%Y-%m-%dT%H:%M:%S-%Z\\\")\\\"\\n\\n# Iterate through a list of datasets, could just as easily loop through the output of zfs list -t filesystem\\nfor DATASET in samba; do\\n  # Get the local latest snapshot fpr diffing\\n  LOCAL_LATEST=\\\"$(zfs list -t snapshot -o name -s creation -H \\\"backup/${DATASET}\\\" | tail -n 1)\\\"\\n\\n  # Check if the local latest snapshot is different than the current state of the filesystem (or if FORCE_BACKUP is set)\\n  if [ \\\"$(zfs diff \\\"${LOCAL_LATEST}\\\" | wc -l)\\\" = \\\"0\\\" ] && [ -z ${FORCE_BACKUP:-} ]; then\\n    echo \\\"Skipping backup of backup/${DATASET} as no files have changed.\\\"\\n    continue\\n  fi\\n\\n  # Take the snapshot\\n  echo \\\"Taking snapshot backup/${DATASET}@${SNAPSHOT_NAME}\\\"\\n  zfs snapshot -r \\\"backup/${DATASET}@${SNAPSHOT_NAME}\\\"\\n\\n  # Get the latest snapshot on the remote side\\n  LATEST_SNAPSHOT=\\\"$(ssh -i \\\"/root/.ssh/id_ed25519_zfsbackup\\\" zfsbackup@backup1 latest)\\\"\\n  \\n  # Send incremental snapshot between the latest on the remote and the one we just took\\n  echo \\\"Sending incremental snapshots between ${LATEST_SNAPSHOT} backup/${DATASET}@${SNAPSHOT_NAME}\\\"\\n  zfs send -RwI \\\"${LATEST_SNAPSHOT}\\\" \\\"backup/${DATASET}@${SNAPSHOT_NAME}\\\" | pv | ssh -i \\\"/root/.ssh/id_ed25519_zfsbackup\\\" zfsbackup@backup1 recv\\ndone\\n\")), mdx(\"p\", null, \"You can test the cron backup by using:\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-bash\"\n  }, \"# Run the script\\n/etc/cron.daily/backup\\n\\n# If the snapshot is new, force send a backup\\nFORCE_BACKUP=true /etc/cron.daily/backup\\n\")), mdx(\"p\", null, \"You can also test it using run-parts:\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-bash\"\n  }, \"# Trigger run-parts for the daily component\\nrun-parts /etc/cron.daily\\n\")), mdx(\"h4\", {\n    \"id\": \"cloud-backups\"\n  }, \"Cloud Backups\"), mdx(\"p\", null, \"Use \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://www.rsync.net/products/zfsintro.html\"\n  }, \"rsync.net\"), \" and send and receive as if you built a second NAS like above!\"), mdx(\"p\", null, \"Or, send snapshots as a blob to \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://www.backblaze.com/cloud-storage\"\n  }, \"BackBlaze B2\"), \". For example:\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-bash\"\n  }, \"# Send a snapshot incrementally and upload it to b2\\nzfs send -vRwI backup/samba@initial backup/samba@next | pv | b2 upload-unbound-stream zfs-backups - backup-samba-initial-backup-samba-next\\n\")), mdx(\"p\", null, \"And receive the snapshot like so:\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-bash\"\n  }, \"# Receive a snapshot from b2\\nb2 download-file-by-name zfs-backups backup-samba-initial-backup-samba-next - | pv | zfs recv -s -v backup/samba\\n\")), mdx(\"h2\", {\n    \"id\": \"monitoring\"\n  }, \"Monitoring\"), mdx(\"p\", null, \"For monitoring my systems, I use \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://prometheus.io/\"\n  }, \"Prometheus\"), \" and \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://grafana.com/\"\n  }, \"Grafana\"), \" exclusively.\\nI won't go into setting up those two services, but I use the following docker-compose.yml for these deployments:\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-yml\"\n  }, \"version: \\\"3.8\\\"\\n\\nservices:\\n  node-exporter:\\n    image: quay.io/prometheus/node-exporter:latest\\n    restart: always\\n    volumes:\\n      - /:/host:ro,rslave\\n    network_mode: host\\n    pid: host\\n    command:\\n      - --path.rootfs=/host\\n  cadvisor:\\n    image: gcr.io/cadvisor/cadvisor:latest\\n    restart: always\\n    volumes:\\n      - /:/rootfs:ro\\n      - /var/run:/var/run:ro\\n      - /sys:/sys:ro\\n      - /var/lib/docker/:/var/lib/docker:ro\\n      - /dev/disk/:/dev/disk:ro\\n    ports:\\n      - 9080:8080\\n    privileged: true\\n    devices:\\n      - /dev/kmsg\\n  smartctl-exporter:\\n    image: prometheuscommunity/smartctl-exporter:latest\\n    restart: always\\n    ports:\\n      - 9633:9633\\n    privileged: true\\n    user: root\\n\")), mdx(\"p\", null, \"This compose file lives in \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"/backup/misc/monitoring\"), \", so it is also retained as part of my backups.\"), mdx(\"p\", null, \"Here are some graphs from Grafana:\"), mdx(\"p\", null, mdx(\"img\", {\n    parentName: \"p\",\n    \"src\": \"/images/zfs-nas/smart-1.png\",\n    \"alt\": \"smart status 1\"\n  }), \"\\n\", mdx(\"img\", {\n    parentName: \"p\",\n    \"src\": \"/images/zfs-nas/smart-2.png\",\n    \"alt\": \"smart status 2\"\n  }), \"\\n\", mdx(\"img\", {\n    parentName: \"p\",\n    \"src\": \"/images/zfs-nas/smart-3.png\",\n    \"alt\": \"smart status 3\"\n  })), mdx(\"p\", null, mdx(\"img\", {\n    parentName: \"p\",\n    \"src\": \"/images/zfs-nas/docker-1.png\",\n    \"alt\": \"docker status 1\"\n  }), \"\\n\", mdx(\"img\", {\n    parentName: \"p\",\n    \"src\": \"/images/zfs-nas/docker-2.png\",\n    \"alt\": \"docker status 2\"\n  })), mdx(\"h2\", {\n    \"id\": \"networking\"\n  }, \"Networking\"), mdx(\"p\", null, \"I utilize \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://www.zerotier.com/\"\n  }, \"ZeroTier\"), \" to maintain a secure network for my devices to communicate on. You can use any method you may choose.\"), mdx(\"h2\", {\n    \"id\": \"acknowledgements\"\n  }, \"Acknowledgements\"), mdx(\"p\", null, \"There are tools available that can help with setting up syncing and replicating ZFS datasets.\\n(see \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://github.com/jimsalterjrs/sanoid\"\n  }, \"Sanoid/Syncoid\"), \"). I'm a firm believer in knowing everything about your data,\\nwhich is why I chose to role things on my own. This guide is to serve as an aide when it comes to choosing what is best\\nfor yourself and your data.\"), mdx(\"p\", null, \"If you have any questions or comments, feel free to shoot me an \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"mailto:me@antoniomika.me\"\n  }, \"email\"), \" or message me on \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://web.libera.chat/#pico.sh\"\n  }, \"IRC\"), \". \"));\n}\n;\nMDXContent.isMDXComponent = true;","frontmatter":{"title":"Roll you own ZFS NAS","tags":["blog"],"emoji":"","link":"","date":"2024-03-13"},"fields":{"dateModified":"21st Mar 2024","slug":"/zfs-nas/"},"tableOfContents":{"items":[{"url":"#background","title":"Background"},{"url":"#its-as-easy-as-3-2-1","title":"It's as easy as 3-2-1"},{"url":"#my-strategy","title":"My strategy"},{"url":"#technologies","title":"Technologies"},{"url":"#backup-setup","title":"Backup Setup","items":[{"url":"#zfs-setup","title":"ZFS Setup"},{"url":"#software-setup","title":"Software Setup"}]},{"url":"#replication-setup","title":"Replication Setup","items":[{"url":"#zfs-snapshots","title":"ZFS Snapshots"},{"url":"#reconciliation","title":"Reconciliation"},{"url":"#automated-backups-with-send-and-receive","title":"Automated backups with send and receive"}]},{"url":"#monitoring","title":"Monitoring"},{"url":"#networking","title":"Networking"},{"url":"#acknowledgements","title":"Acknowledgements"}]},"parent":{"__typename":"File","name":"zfs-nas","fileName":"zfs-nas.md"}}},"pageContext":{"id":"d2dbe321-8325-581c-89d9-5a5b78b1c0d9","previous":{"id":"938475a9-8524-575e-a67e-8b4afc08093b","frontmatter":{"title":"Shell Sharing using TCP Sockets","tags":["blog"]},"fields":{"slug":"/shell-sharing/"}},"next":{"id":"e77a3740-52b8-50c9-998d-9fba78f5f155","frontmatter":{"title":"Hello World!","tags":["blog"]},"fields":{"slug":"/hello-world/"}},"hasUntagged":false,"basePath":"/"}},"staticQueryHashes":["1322576299","1437003973","467212769","467212769"]}